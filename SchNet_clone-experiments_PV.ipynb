{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11d618c1-7bd7-4b54-b5c6-cd8fab531f12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_scatter import scatter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from DataClasses import lmdb_dataset, Dataset\n",
    "from ModelFunctions import train, evaluate, inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ff033de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_reshape(tensor):\n",
    "    return torch.reshape(tensor, (tensor.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51d80a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(system):\n",
    "    #spherical_radii = torch.Tensor(system['spherical_domain_radii'])\n",
    "    #spherical_radii = my_reshape(spherical_radii)\n",
    "    \n",
    "#     tags = system['tags'].long().to(device)\n",
    "#     tags = F.one_hot(tags, num_classes=3)\n",
    "    \n",
    "    atom_numbers = system['atomic_numbers'].long().to(device)\n",
    "    atom_numbers = F.one_hot(atom_numbers, num_classes=100).float()\n",
    "    \n",
    "#     voronoi_volumes = system['voronoi_volumes'].float().to(device)\n",
    "#     voronoi_volumes = my_reshape(voronoi_volumes)\n",
    "    \n",
    "    atom_features = (atom_numbers,) #tags, voronoi_volumes)#, spherical_radii)\n",
    "    atom_embeds = torch.cat(atom_features, 1)\n",
    "    \n",
    "    edge_index = system['edge_index'].long().to(device)\n",
    "    \n",
    "    distances = system['distances'].float().to(device)\n",
    "    distances = my_reshape(distances)\n",
    "    \n",
    "#     angles = system['contact_solid_angles'].float().to(device)\n",
    "#     angles = my_reshape(angles)\n",
    "    \n",
    "    edge_features = (distances,)# angles)\n",
    "    \n",
    "    edges_embeds = torch.cat(edge_features, 1)\n",
    "    \n",
    "    \n",
    "    return Data(x=atom_embeds.to(device), edge_index=edge_index.to(device), edge_attr=edges_embeds.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28f277f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianSmearing(nn.Module):\n",
    "    def __init__(self, start=0.0, stop=8.0, num_gaussians=150):\n",
    "        super(GaussianSmearing, self).__init__()\n",
    "        offset = torch.linspace(start, stop, num_gaussians)\n",
    "        self.coeff = -0.5 / (offset[1] - offset[0]).item()**2\n",
    "        self.register_buffer('offset', offset)\n",
    "\n",
    "    def forward(self, dist):\n",
    "        dist = dist.view(-1, 1) - self.offset.view(1, -1)\n",
    "        return torch.exp(self.coeff * torch.pow(dist, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b069c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShiftedSoftplus(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ShiftedSoftplus, self).__init__()\n",
    "        self.shift = torch.log(torch.tensor(2.0)).item()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softplus(x) - self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adc9d0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFconv(MessagePassing):\n",
    "    def __init__(self, dim_hidden, dim_edge):   #dim_edge можно и не передавать\n",
    "        super(CFconv, self).__init__(aggr='add')\n",
    "        self.rbf = GaussianSmearing(num_gaussians=smearing['rbf'])\n",
    "        #self.sa_bins = GaussianSmearing(start=0.0, stop=50.0, num_gaussians=smearing['sa_bins']) #кладём телесные углы в бины\n",
    "        self.blocks = nn.Sequential(nn.Linear(smearing['rbf']+smearing['sa_bins'], dim_hidden, bias=True),\n",
    "                                   ShiftedSoftplus(),\n",
    "                                   nn.Linear(dim_hidden, dim_hidden, bias=True),\n",
    "                                   ShiftedSoftplus())\n",
    "        self.lin_phi = torch.nn.Linear(dim_hidden, dim_hidden, bias=False)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.blocks[0].weight)\n",
    "        self.blocks[0].bias.data.fill_(0)\n",
    "        torch.nn.init.xavier_uniform_(self.blocks[2].weight)\n",
    "        self.blocks[0].bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = batch['x']\n",
    "        edge_index = batch['edge_index']\n",
    "        rbf_dist = self.rbf(batch['edge_attr'][:, 0])\n",
    "        #bins_angles = self.sa_bins(batch['edge_attr'][:, 1])\n",
    "        #edge_attr = torch.cat((rbf_dist, bins_angles), 1)\n",
    "        edge_attr = rbf_dist\n",
    "        edge_attr = self.blocks(edge_attr)\n",
    "        \n",
    "    \n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr, size=None)\n",
    "\n",
    "    def message(self, x, x_i, x_j, edge_attr):\n",
    "        new_edges = self.lin_phi(edge_attr)\n",
    "        hd_product = x_j * new_edges\n",
    "        return hd_product\n",
    "        \n",
    "    def update(self, aggr_out):\n",
    "\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da342621",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interaction(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_hidden, dim_edge):\n",
    "        super().__init__()\n",
    "        self.atom_wise_64_1 = nn.Linear(dim_hidden, dim_hidden, bias=True)\n",
    "        self.cfconv = CFconv(dim_hidden, dim_edge)\n",
    "        self.atom_wise_64_2 = nn.Linear(dim_hidden, dim_hidden, bias=True)\n",
    "        self.shifted_softplus = ShiftedSoftplus()\n",
    "        self.atom_wise_64_3 = nn.Linear(dim_hidden, dim_hidden, bias=True)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.atom_wise_64_1.weight)\n",
    "        self.atom_wise_64_1.bias.data.fill_(0)\n",
    "        torch.nn.init.xavier_uniform_(self.atom_wise_64_2.weight)\n",
    "        self.atom_wise_64_2.bias.data.fill_(0)\n",
    "        torch.nn.init.xavier_uniform_(self.atom_wise_64_3.weight)\n",
    "        self.atom_wise_64_3.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        x_input = batch['x'].clone().detach()\n",
    "        batch['x'] = self.atom_wise_64_1(batch['x'])\n",
    "        conved = self.cfconv(batch)\n",
    "        conved = self.atom_wise_64_2(conved)\n",
    "        ssp = self.shifted_softplus(conved)\n",
    "        v = self.atom_wise_64_3(ssp)\n",
    "        batch['x'] = x_input + v\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e1232c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_atom=103, dim_edge=2, dim_hidden=64, num_int=3):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(dim_atom, dim_hidden)\n",
    "        \n",
    "        int_blocks = []\n",
    "        for i in range(num_int):\n",
    "            int_blocks.append(Interaction(dim_hidden, dim_edge))\n",
    "        self.interactions = nn.Sequential(*int_blocks)\n",
    "        self.shifted_softplus = ShiftedSoftplus()\n",
    "        self.atom_wise_32 = nn.Linear(dim_hidden, 32, bias=True)\n",
    "        self.atom_wise_1 = nn.Linear(32, 1, bias=True)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.atom_wise_32.weight)\n",
    "        self.atom_wise_32.bias.data.fill_(0)\n",
    "        torch.nn.init.xavier_uniform_(self.atom_wise_1.weight)\n",
    "        self.atom_wise_1.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        batch['x'] = self.embedding(batch['x'])\n",
    "        batch = self.interactions(batch)\n",
    "        x_32 = self.atom_wise_32(batch['x'])\n",
    "        x_32 = self.shifted_softplus(x_32)\n",
    "        energies = self.atom_wise_1(x_32)\n",
    "        energy = scatter(energies, batch['batch'], dim=0, reduce='sum')\n",
    "        \n",
    "        return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1316697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "batch_size = 50\n",
    "num_workers = 0\n",
    "\n",
    "features_cols = ['atomic_numbers', 'edge_index_new', 'distances_new', \n",
    "                 'contact_solid_angles', 'tags', 'voronoi_volumes', 'spherical_domain_radii'] #бесполезный массив\n",
    "\n",
    "target_col = 'y_relaxed'\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "smearing = {'rbf' : 300, 'sa_bins' : 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3099636a-c3a8-4bf5-86f8-4983eba165c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#чтобы тензор по умолчанию заводился на куде\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    print('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d2b308d-ffa4-4f48-adab-e218d7da0046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adf8ed5d-ba8c-44e7-a343-1e7092ffca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#инициализируем тренировочный датасети и тренировочный итератор\n",
    "train_dataset_file_path= os.path.expanduser(\"../../ocp_datasets/data/is2re/10k/train/data_mod.lmdb\")\n",
    "\n",
    "training_set = Dataset(train_dataset_file_path, features_cols, target_col, preprocessing=preprocessing)\n",
    "training_generator = DataLoader(training_set, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae852e60-b1ac-4a48-8120-af94ecec6967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#инициализируем валидационный датасет и валидационный итератор\n",
    "val_dataset_file_path = os.path.expanduser(\"../../ocp_datasets/data/is2re/all/val_ood_both/data_mod.lmdb\")\n",
    "\n",
    "valid_set = Dataset(val_dataset_file_path, features_cols, target_col, preprocessing=preprocessing)\n",
    "valid_generator = DataLoader(valid_set, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f64a1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item: 0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    lmdb_dataset(train_dataset_file_path).describe()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca6c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for num_int in range(15):\n",
    "    \n",
    "        #model\n",
    "    model = ConvNN(dim_atom=training_set[0][0]['x'].shape[1], dim_edge=training_set[0][0]['edge_attr'].shape[1], num_int=num_int)\n",
    "\n",
    "    #optimizer and loss\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "    #переносим на куду если она есть\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    loss = []\n",
    "    loss_eval = []\n",
    "    \n",
    "    timestamp = str(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "\n",
    "    writer = None\n",
    "\n",
    "    print(timestamp)\n",
    "    print(f'Start training model {num_int}')\n",
    "    epochs=25\n",
    "    for i in range(epochs):\n",
    "        loss.append(train(model, training_generator, optimizer, criterion, epoch=i, writer=writer, device=device))\n",
    "        loss_eval.append(evaluate(model, valid_generator, criterion, epoch=i, writer=writer, device=device))\n",
    "    with open(f'log_{num_int}.txt', 'w') as f:\n",
    "        f.write(' '.join((str(num_int), str(loss_eval), str(min(loss_eval)))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocp_models",
   "language": "python",
   "name": "ocp_models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
